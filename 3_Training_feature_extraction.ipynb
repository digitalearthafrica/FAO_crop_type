{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcefb35-29c7-4b46-ab8e-05aa785d2d14",
   "metadata": {},
   "source": [
    "# Training feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b64ce-04dd-4bcd-9c66-76b8a3e3c880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "Based on our initial exploration and understanding of the information that can be provided by the Earth observation (EO) data, we want to use both spectral and temporal EO measurements to classify crop types. We can also use information about the landscape, such as slope, as a predictor for crop type.\n",
    "\n",
    "In a supervised machine learning approach, we will first build a labelled training dataset, combining the crop labels and their associated EO-derived information that can be extracted from the DE Africa platform. The training dataset is transformed into a set of features that can be understood by the machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae751d-1f07-4135-8627-091b4fe0fdf7",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Different machine learning models and implementations require different types of training data. We use [`scikit-learn`](https://scikit-learn.org/stable/), a powerful Python libary with a comprehensive set of machine learning algorithms and tools.\n",
    "\n",
    "In this notebook, we demonstrate how to use extract data from the DE Africa platform, combine them with crop labels, and transform them into a set of features that will be used to train a machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c45f81-7fc4-44b9-a3ed-531767565f7b",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a91d58-e1f7-4c95-8a0c-7d7662ea22a8",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad68cd99-a9e3-49bc-9dec-13807742875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.3-CAPI-1.16.1). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from datacube.utils import geometry\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from feature_collection import feature_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c0b36d-a27e-4797-8b96-4956e97dae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data and results directories if they don't exist\n",
    "input_folder=\"Data\"\n",
    "output_folder=\"Results\"\n",
    "output_crs=\"EPSG:6933\"\n",
    "if not os.path.exists(input_folder):\n",
    "    os.makedirs(input_folder)\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a144d88-e16b-4c03-a1d6-d793e719c34f",
   "metadata": {},
   "source": [
    "## Load training crop labels\n",
    "\n",
    "We start with the cleaned and merged crop labels, which include four crop classes: Maize, Sesame, Soy, and Others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030dbe96-8cc9-46e5-a35f-2de2e84f27ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>Maize</td>\n",
       "      <td>POLYGON ((3280425.252 -2409501.264, 3280415.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>Maize</td>\n",
       "      <td>POLYGON ((3280689.817 -2409502.893, 3280704.76...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>Sesame</td>\n",
       "      <td>POLYGON ((3529816.669 -2109463.643, 3529807.92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>Maize</td>\n",
       "      <td>POLYGON ((3280542.444 -2408144.561, 3280512.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>Maize</td>\n",
       "      <td>POLYGON ((3301561.584 -2393086.954, 3301569.64...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year Crop_type                                           geometry\n",
       "0  2021     Maize  POLYGON ((3280425.252 -2409501.264, 3280415.15...\n",
       "1  2021     Maize  POLYGON ((3280689.817 -2409502.893, 3280704.76...\n",
       "2  2021    Sesame  POLYGON ((3529816.669 -2109463.643, 3529807.92...\n",
       "3  2021     Maize  POLYGON ((3280542.444 -2408144.561, 3280512.19...\n",
       "4  2021     Maize  POLYGON ((3301561.584 -2393086.954, 3301569.64..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Point to crop type training data\n",
    "path= os.path.join(input_folder,\"Cash_crop_type_subset_single_crops_merged.shp\")\n",
    "\n",
    "# Load input data and project\n",
    "single_crops_subset = gpd.read_file(path).to_crs(output_crs)\n",
    "single_crops_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872259a-6ffa-4949-b3bc-c94b99cd6406",
   "metadata": {},
   "source": [
    "### Map crop types to numeric classes for prediction\n",
    "\n",
    "The crop type labels need to be transformed into numbers for them to work with the ML algorithm. \n",
    "We will save the mapping as a JSON file so the predictions can be transformed back into the crop type labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0956ac4e-94b0-4eaf-a704-af3527b353c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Dictionary:\n",
      "{'Maize': 0, 'Others': 1, 'Sesame': 2, 'Soy': 3}\n"
     ]
    }
   ],
   "source": [
    "# Select field to label\n",
    "field = \"Crop_type\"\n",
    "\n",
    "# Fit label encoder to match classes to numeric labels\n",
    "le = LabelEncoder()\n",
    "le.fit(single_crops_subset[field])\n",
    "\n",
    "# Get a list of the crop types\n",
    "classes = list(le.classes_)\n",
    "\n",
    "# Assign numeric label for each class\n",
    "single_crops_subset[\"label\"] = le.transform(single_crops_subset[field])\n",
    "\n",
    "# Create a dictionary mapping classes to numeric labels\n",
    "class_dictionary = {crop_class: int(le.transform([crop_class])[0]) for crop_class in classes}\n",
    "print(\"Class Dictionary:\")\n",
    "print(class_dictionary)\n",
    "\n",
    "# Export class dictionary\n",
    "with open(os.path.join(output_folder,\"class_labels.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(class_dictionary, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c782d1-4a22-4b09-9c1b-ba9b9d988774",
   "metadata": {},
   "source": [
    "## Prepare query for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6230a-8999-4a1d-ba2e-c07554e52967",
   "metadata": {},
   "source": [
    "The machine learning process uses median and geomedian products to ensure inputs are not affected by cloud cover. Using your understanding of when the data was collected, you will need to manually define the time periods you want to generate cloud-free median composites for. \n",
    "\n",
    "The next step is to manually configure the time periods for the composites you want to use. There are four types:\n",
    "* `annual_geomedian_times`: Used to specify which annual geomedian to use\n",
    "* `semiannual_geomedian_times`: Used to specify which semi-annual geomedians to use\n",
    "* `s1_time_ranges`: Used to specify the periods for calculating medians for Sentinel-1\n",
    "* `time_ranges`: Used for to specify the periods for calculating all other medians and geomedians\n",
    "\n",
    "Each of these is configured as a dictionary, where the key indicates the label to attribute to the data (used to distinguish features loaded for different time periods) and the value corresponds to the date or date range associated with that time period (in \"YYYY-MM-DD\" format).\n",
    "\n",
    "For data collected in April of 2022, here is an example:\n",
    "\n",
    "```\n",
    "annual_geomedian_times = {\n",
    "    \"annual_2021\": \"2021-01-01\",\n",
    "}\n",
    "\n",
    "semiannual_geomedian_times = {\n",
    "    \"semiannual_2021_01\": \"2021-01-01\",\n",
    "    \"semiannual_2021_06\": \"2021-06-01\",\n",
    "}\n",
    "\n",
    "s1_time_ranges = {\n",
    "    \"Q3_2021\": slice(\"2021-08-01\", \"2022-10-31\"),\n",
    "    \"Q4_2021\": slice(\"2021-11-01\", \"2022-01-31\"),\n",
    "}\n",
    "\n",
    "time_ranges = {\n",
    "    \"Q3_2021\": slice(\"2021-08-01\", \"2022-10-31\"),\n",
    "    \"Q4_2021\": slice(\"2021-11-01\", \"2022-01-31\"),\n",
    "    \"Q1_2022\": slice(\"2022-02-01\", \"2022-04-30\"),\n",
    "}\n",
    "```\n",
    "\n",
    "The default setting is to load:\n",
    "\n",
    "* One Digital Earth Africa annual geomedian for 2021\n",
    "* Two Digital Earth Africa semi-annual geomedians, one in the first half of 2021, one in the second half of 2021\n",
    "* Two quarterly medians for Sentinel-1, one from August 2021 to October 2021, and the other from November 2021 to January 2022\n",
    "* Three quarterly medians for all other products, one from August 2021 to October 2021, one from November 2021 to January 2022, and one from February 2022 to April 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c618c6a2-079a-4a1e-9131-9e7059df2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_geomedian_times = {\n",
    "    \"annual_2021\": \"2021-01-01\",\n",
    "}\n",
    "\n",
    "semiannual_geomedian_times = {\n",
    "    \"semiannual_2021_07\": \"2021-07-01\",\n",
    "    \"semiannual_2022_01\": \"2022-01-01\",\n",
    "}\n",
    "\n",
    "# s1_time_ranges = {\n",
    "#     \"Q4_2021\": slice(\"2021-10-01\", \"2021-12-31\"),\n",
    "# }\n",
    "\n",
    "time_ranges = {\n",
    "    \"Q4_2021\": slice(\"2021-10-01\", \"2021-12-31\"),\n",
    "    \"Q1_2022\": slice(\"2022-01-01\", \"2022-03-31\"),\n",
    "    \"Q2_2022\": slice(\"2022-04-01\", \"2022-06-30\"),\n",
    "    \"Q3_2022\": slice(\"2022-07-01\", \"2022-09-30\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b029f-7615-49de-b866-4fd6a9b4c6d1",
   "metadata": {},
   "source": [
    "We also need to set the spatial requirements and combine all parameters into a query dictionary. This query dictionay is saved and will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1773e87d-eac5-43e7-b5d7-ec207ccafe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (-10, 10)\n",
    "query = {\n",
    "    \"annual_geomedian_times\": annual_geomedian_times,\n",
    "    \"semiannual_geomedian_times\": semiannual_geomedian_times,\n",
    "#     \"s1_time_ranges\": s1_time_ranges,\n",
    "    \"time_ranges\": time_ranges,\n",
    "    \"resolution\": resolution,\n",
    "    \"output_crs\": output_crs,\n",
    "}\n",
    "\n",
    "# Export query to pickle file for future re-use\n",
    "output_query=os.path.join(output_folder,'query.pickle')\n",
    "with open(output_query, 'wb') as f:\n",
    "    pickle.dump(query, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed5f26-17d9-47b2-8158-5b1e09f3f7e9",
   "metadata": {},
   "source": [
    "## Collect training data\n",
    "\n",
    "We use the `collect_training_data()` function to extract data from the DE Africa platform.\n",
    "By default, the method below will run in parallel mode, which decreases the amount of time to run feature extraction for each geometry. \n",
    "\n",
    "We also use the `feature_layers()` function defined in `feature_collection.py`. An error may occur if the `feature_layers()` is not defined properly.\n",
    "\n",
    "### Debugging\n",
    "When testing, it is suggested you set `parallel = False` below to switch back to serial mode. \n",
    "\n",
    "You can also set `gdf = single_crops_subset.iloc[0:5, :].copy()` in the function call to only run on the first five geometries.\n",
    "\n",
    "> This step may take a few hours to run over a thousand polygons, depend on the number of features to extract and how much processing is required to obtain the measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7239d8-1fa2-469b-a38c-ed39db1c9c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 4\n"
     ]
    }
   ],
   "source": [
    "# Set parallel mode on or off (set to False if testing a new feature extraction function).\n",
    "parallel = True\n",
    "\n",
    "if parallel:\n",
    "    ncpus = round(get_cpu_quota())\n",
    "else:\n",
    "    ncpus = 1\n",
    "    \n",
    "print(\"ncpus = \" + str(ncpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c18dda35-d4d3-4d61-84ca-f1736c470ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600c7413e1c94f1ca9d0d16d44db9c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/rasterio/warp.py:346: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  _reproject(\n",
      "/usr/local/lib/python3.8/dist-packages/rasterio/warp.py:346: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  _reproject(\n",
      "/usr/local/lib/python3.8/dist-packages/rasterio/warp.py:346: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  _reproject(\n",
      "/usr/local/lib/python3.8/dist-packages/rasterio/warp.py:346: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  _reproject(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 1 = 0.0 %\n",
      "Removed 0 rows wth NaNs &/or Infs\n",
      "Output shape:  (31739, 125)\n"
     ]
    }
   ],
   "source": [
    "# Collect the training data\n",
    "column_names, model_input = collect_training_data(\n",
    "    gdf=single_crops_subset,\n",
    "    dc_query=query,\n",
    "    ncpus=ncpus,\n",
    "    field=\"label\",\n",
    "    zonal_stats=None,\n",
    "    feature_func=feature_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14f467-f012-48ae-828a-c2d3d5f0f82b",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Finally, we export the training data to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "349687e0-f375-4f87-97a8-20ed44711e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the name and location of the output file\n",
    "output_file = os.path.join(output_folder,'single_crops_merged_training_features_2021_all.csv')\n",
    "\n",
    "#convert to a dataframe and save as a csv file\n",
    "model_input_df = pd.DataFrame(model_input, columns=column_names)\n",
    "model_input_df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
