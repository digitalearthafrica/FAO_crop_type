{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcefb35-29c7-4b46-ab8e-05aa785d2d14",
   "metadata": {},
   "source": [
    "# Training feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b64ce-04dd-4bcd-9c66-76b8a3e3c880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "Based on our initial exploration and understanding of the information that can be provided by the Earth observation (EO) data, we want to use both spectral and temporal EO measurements to classify crop types. We can also use information about the landscape, such as slope, as a predictor for crop type.\n",
    "\n",
    "In a supervised machine learning approach, we will first build a labelled training dataset, combining the crop labels and their associated EO-derived information that can be extracted from the DE Africa platform. The training dataset is transformed into a set of features that can be understood by the machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae751d-1f07-4135-8627-091b4fe0fdf7",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Different machine learning models and implementations require different types of training data. We use [`scikit-learn`](https://scikit-learn.org/stable/), a powerful Python libary with a comprehensive set of machine learning algorithms and tools.\n",
    "\n",
    "In this notebook, we demonstrate how to use extract data from the DE Africa platform, combine them with crop labels, and transform them into a set of features that will be used to train a machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c45f81-7fc4-44b9-a3ed-531767565f7b",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a91d58-e1f7-4c95-8a0c-7d7662ea22a8",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad68cd99-a9e3-49bc-9dec-13807742875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.3-CAPI-1.16.1). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from feature_collection import feature_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c0b36d-a27e-4797-8b96-4956e97dae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data and results directories if they don't exist\n",
    "input_folder=\"Results/Labels\"\n",
    "output_folder=\"Results/Model\"\n",
    "output_crs=\"EPSG:6933\"\n",
    "if not os.path.exists(input_folder):\n",
    "    os.makedirs(input_folder)\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a144d88-e16b-4c03-a1d6-d793e719c34f",
   "metadata": {},
   "source": [
    "## Load training crop labels\n",
    "\n",
    "We start with the cleaned and merged crop labels, which include four crop classes: Maize, Sesame, Soy, and Others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030dbe96-8cc9-46e5-a35f-2de2e84f27ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>Maize</td>\n",
       "      <td>POLYGON ((3280425.252 -2409501.264, 3280415.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>Maize</td>\n",
       "      <td>POLYGON ((3280689.817 -2409502.893, 3280704.76...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>Sesame</td>\n",
       "      <td>POLYGON ((3529816.669 -2109463.643, 3529807.92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>Maize</td>\n",
       "      <td>POLYGON ((3280542.444 -2408144.561, 3280512.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>Maize</td>\n",
       "      <td>POLYGON ((3301561.584 -2393086.954, 3301569.64...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year Crop_type                                           geometry\n",
       "0  2021     Maize  POLYGON ((3280425.252 -2409501.264, 3280415.15...\n",
       "1  2021     Maize  POLYGON ((3280689.817 -2409502.893, 3280704.76...\n",
       "2  2021    Sesame  POLYGON ((3529816.669 -2109463.643, 3529807.92...\n",
       "3  2021     Maize  POLYGON ((3280542.444 -2408144.561, 3280512.19...\n",
       "4  2021     Maize  POLYGON ((3301561.584 -2393086.954, 3301569.64..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Point to crop type training data\n",
    "path= os.path.join(input_folder,\"Cash_crop_type_subset_single_crops_merged.shp\")\n",
    "\n",
    "# Load input data and project\n",
    "single_crops_subset = gpd.read_file(path).to_crs(output_crs)\n",
    "single_crops_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872259a-6ffa-4949-b3bc-c94b99cd6406",
   "metadata": {},
   "source": [
    "### Map crop types to numeric classes for prediction\n",
    "\n",
    "The crop type labels need to be transformed into numbers for them to work with the ML algorithm. \n",
    "We will save the mapping as a JSON file so the predictions can be transformed back into the crop type labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0956ac4e-94b0-4eaf-a704-af3527b353c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Dictionary:\n",
      "{'Maize': 0, 'Others': 1, 'Sesame': 2, 'Soy': 3}\n"
     ]
    }
   ],
   "source": [
    "# Select field to label\n",
    "field = \"Crop_type\"\n",
    "\n",
    "# Fit label encoder to match classes to numeric labels\n",
    "le = LabelEncoder()\n",
    "le.fit(single_crops_subset[field])\n",
    "\n",
    "# Get a list of the crop types\n",
    "classes = list(le.classes_)\n",
    "\n",
    "# Assign numeric label for each class\n",
    "single_crops_subset[\"label\"] = le.transform(single_crops_subset[field])\n",
    "\n",
    "# Create a dictionary mapping classes to numeric labels\n",
    "class_dictionary = {crop_class: int(le.transform([crop_class])[0]) for crop_class in classes}\n",
    "print(\"Class Dictionary:\")\n",
    "print(class_dictionary)\n",
    "\n",
    "# Export class dictionary\n",
    "with open(os.path.join(output_folder,\"class_labels.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(class_dictionary, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c782d1-4a22-4b09-9c1b-ba9b9d988774",
   "metadata": {},
   "source": [
    "## Prepare query for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6230a-8999-4a1d-ba2e-c07554e52967",
   "metadata": {},
   "source": [
    "Our machine learning approach uses measurements from statistical summary products as input features to ensure data are not affected by cloud cover in individual images. Temporal coverage of these products should be defined based on collection time of the labeled data and the crop year to map.\n",
    "\n",
    "Specifically, we define below time windows for three different input datasets:\n",
    "\n",
    "* `semiannual_geomad_times`: for Semi-annual [GeoMAD product](https://docs.digitalearthafrica.org/en/latest/data_specs/GeoMAD_specs.html)\n",
    "* `monthly_ndvi_time_range`: for time range of [Monthly NDVI](https://docs.digitalearthafrica.org/en/latest/data_specs/NDVI_Anomaly_specs.html)\n",
    "* `ls_fc_cover_times`: for time ranges of [Fractional Cover](https://docs.digitalearthafrica.org/en/latest/data_specs/Fractional_Cover_specs.html)\n",
    "\n",
    "Each of these is configured as a dictionary, where the key indicates the feature name and the value is a date or date range for the data query (in \"YYYY-MM-DD\" format).\n",
    "\n",
    "The crop labels we use were collected in April 2022, therefore we use time periods that overlap with the 2021/2022 crop season.\n",
    "\n",
    "The example list of parameters below retrieves:\n",
    "\n",
    "* Two Digital Earth Africa Semi-annual [GeoMAD](https://docs.digitalearthafrica.org/en/latest/data_specs/GeoMAD_specs.html), for the second half of 2021 and the first half of 2022.\n",
    "* Digital Earth Africa [Monthly NDVI](https://docs.digitalearthafrica.org/en/latest/data_specs/NDVI_Anomaly_specs.html), from Oct 2021 to Sep 2022.\n",
    "* Four quarterly medians of [Fractional Cover](https://docs.digitalearthafrica.org/en/latest/data_specs/Fractional_Cover_specs.html), from Oct 2021 to Sep 2022.\n",
    "\n",
    "```\n",
    "semiannual_geomad_times = {\n",
    "    \"semiannual_2021_07\": \"2021-07-01\",\n",
    "    \"semiannual_2022_01\": \"2022-01-01\",\n",
    "}\n",
    "\n",
    "monthly_ndvi_time_range=(\"2021-10\",\"2022-09\")\n",
    "\n",
    "ls_fc_cover_times = {\n",
    "    \"Q4_2021\": slice(\"2021-10-01\", \"2021-12-31\"),\n",
    "    \"Q1_2022\": slice(\"2022-01-01\", \"2022-03-31\"),\n",
    "    \"Q2_2022\": slice(\"2022-04-01\", \"2022-06-30\"),\n",
    "    \"Q3_2022\": slice(\"2022-07-01\", \"2022-09-30\"),\n",
    "}\n",
    "```\n",
    "\n",
    "In addition to the time series measurements, we also use slope derived from the Digital Elevation Model (DEM) as an input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c618c6a2-079a-4a1e-9131-9e7059df2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "semiannual_geomad_times = {\n",
    "    \"semiannual_2021_07\": \"2021-07-01\",\n",
    "    \"semiannual_2022_01\": \"2022-01-01\",\n",
    "}\n",
    "\n",
    "monthly_ndvi_time_range=(\"2021-10\",\"2022-09\")\n",
    "\n",
    "ls_fc_cover_times = {\n",
    "    \"Q4_2021\": slice(\"2021-10-01\", \"2021-12-31\"),\n",
    "    \"Q1_2022\": slice(\"2022-01-01\", \"2022-03-31\"),\n",
    "    \"Q2_2022\": slice(\"2022-04-01\", \"2022-06-30\"),\n",
    "    \"Q3_2022\": slice(\"2022-07-01\", \"2022-09-30\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b029f-7615-49de-b866-4fd6a9b4c6d1",
   "metadata": {},
   "source": [
    "We also need to set the spatial requirements and combine all parameters into a query dictionary. This query dictionay is saved and will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1773e87d-eac5-43e7-b5d7-ec207ccafe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (-10, 10)\n",
    "query = {\n",
    "    #\"annual_geomad_times\": annual_geomad_times,\n",
    "    \"semiannual_geomad_times\": semiannual_geomad_times,\n",
    "    \"monthly_ndvi_time_range\":monthly_ndvi_time_range,\n",
    "    \"ls_fc_cover_times\": ls_fc_cover_times,\n",
    "    \n",
    "    \"resolution\": resolution,\n",
    "    \"output_crs\": output_crs,\n",
    "}\n",
    "\n",
    "# Export query to pickle file for future re-use\n",
    "output_query=os.path.join(output_folder,'query.pickle')\n",
    "with open(output_query, 'wb') as f:\n",
    "    pickle.dump(query, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed5f26-17d9-47b2-8158-5b1e09f3f7e9",
   "metadata": {},
   "source": [
    "## Collect training data\n",
    "\n",
    "We use the `collect_training_data()` function to extract data from the DE Africa platform.\n",
    "By default, the method below will run in parallel mode, which decreases the amount of time to run feature extraction for each geometry. \n",
    "\n",
    "We also use the `feature_layers()` function defined in `feature_collection.py`. An error may occur if the `feature_layers()` is not defined properly.\n",
    "\n",
    "### Debugging\n",
    "When testing, it is suggested you set `parallel = False` below to switch back to serial mode. \n",
    "\n",
    "You can also set `gdf = single_crops_subset.iloc[0:5, :].copy()` in the function call to only run on the first five geometries.\n",
    "\n",
    "> This step may take a few hours to run over a thousand polygons, depend on the number of features to extract and how much processing is required to obtain the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7239d8-1fa2-469b-a38c-ed39db1c9c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 4\n"
     ]
    }
   ],
   "source": [
    "# Set parallel mode on or off (set to False if testing a new feature extraction function).\n",
    "parallel = True\n",
    "\n",
    "if parallel:\n",
    "    ncpus = round(get_cpu_quota())\n",
    "else:\n",
    "    ncpus = 1\n",
    "    \n",
    "print(\"ncpus = \" + str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689050b1-2dc8-4aaf-8100-c5fe24ec447c",
   "metadata": {},
   "source": [
    "For testing this workflow, we extract data over a random subsample of the labeled polygons. To do this we set `subsample = True`. In this instance features will be extracted over 10 randomly selected polygons. \n",
    "\n",
    "Collecting training data over the entire area would require you to set `subsample = False`.\n",
    "\n",
    "> **Note:** In the following notebooks, we use the pre-loaded training data that has been collected over the entire test area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ab5112-fa7f-49c3-9333-fd94766aff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = True\n",
    "if subsample: \n",
    "    n_subset = 10\n",
    "    n_sample = len(single_crops_subset)\n",
    "    if n_sample > n_subset:\n",
    "        subset = np.random.choice(\n",
    "            single_crops_subset.index.values, n_subset)\n",
    "        single_crops_subset = single_crops_subset.loc[subset]\n",
    "        single_crops_subset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18dda35-d4d3-4d61-84ca-f1736c470ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4c7283093a4fc285b24d155545484e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 1 = 0.0 %\n",
      "Removed 0 rows wth NaNs &/or Infs\n",
      "Output shape:  (686, 66)\n",
      "CPU times: user 92.2 ms, sys: 42 ms, total: 134 ms\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Collect the training data\n",
    "column_names, model_input = collect_training_data(\n",
    "    gdf=single_crops_subset,\n",
    "    dc_query=query,\n",
    "    ncpus=ncpus,\n",
    "    field=\"label\",\n",
    "    zonal_stats=None,\n",
    "    feature_func=feature_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af06c1d8-ec07-4958-90f0-eb0faaadf616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training features collected: 65\n",
      "List of training features collected: ['blue_s2_semiannual_2021_07', 'green_s2_semiannual_2021_07', 'red_s2_semiannual_2021_07', 'nir_1_s2_semiannual_2021_07', 'nir_2_s2_semiannual_2021_07', 'swir_1_s2_semiannual_2021_07', 'swir_2_s2_semiannual_2021_07', 'red_edge_1_s2_semiannual_2021_07', 'red_edge_2_s2_semiannual_2021_07', 'red_edge_3_s2_semiannual_2021_07', 'smad_s2_semiannual_2021_07', 'emad_s2_semiannual_2021_07', 'bcmad_s2_semiannual_2021_07', 'NDVI_s2_semiannual_2021_07', 'LAI_s2_semiannual_2021_07', 'SAVI_s2_semiannual_2021_07', 'MSAVI_s2_semiannual_2021_07', 'MNDWI_s2_semiannual_2021_07', 'blue_s2_semiannual_2022_01', 'green_s2_semiannual_2022_01', 'red_s2_semiannual_2022_01', 'nir_1_s2_semiannual_2022_01', 'nir_2_s2_semiannual_2022_01', 'swir_1_s2_semiannual_2022_01', 'swir_2_s2_semiannual_2022_01', 'red_edge_1_s2_semiannual_2022_01', 'red_edge_2_s2_semiannual_2022_01', 'red_edge_3_s2_semiannual_2022_01', 'smad_s2_semiannual_2022_01', 'emad_s2_semiannual_2022_01', 'bcmad_s2_semiannual_2022_01', 'NDVI_s2_semiannual_2022_01', 'LAI_s2_semiannual_2022_01', 'SAVI_s2_semiannual_2022_01', 'MSAVI_s2_semiannual_2022_01', 'MNDWI_s2_semiannual_2022_01', 'ndvi_mean_0', 'ndvi_mean_1', 'ndvi_mean_2', 'ndvi_mean_3', 'ndvi_mean_4', 'ndvi_mean_5', 'ndvi_mean_6', 'ndvi_mean_7', 'ndvi_mean_8', 'ndvi_mean_9', 'ndvi_mean_10', 'ndvi_mean_11', 'bs_mean_Q4_2021', 'pv_mean_Q4_2021', 'npv_mean_Q4_2021', 'ue_mean_Q4_2021', 'bs_mean_Q1_2022', 'pv_mean_Q1_2022', 'npv_mean_Q1_2022', 'ue_mean_Q1_2022', 'bs_mean_Q2_2022', 'pv_mean_Q2_2022', 'npv_mean_Q2_2022', 'ue_mean_Q2_2022', 'bs_mean_Q3_2022', 'pv_mean_Q3_2022', 'npv_mean_Q3_2022', 'ue_mean_Q3_2022', 'slope']\n"
     ]
    }
   ],
   "source": [
    "# Print the list of features collected\n",
    "print(\"# of training features collected:\", len(column_names)-1)\n",
    "print(\"List of training features collected:\", column_names[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14f467-f012-48ae-828a-c2d3d5f0f82b",
   "metadata": {},
   "source": [
    "## Export training data\n",
    "\n",
    "Finally, we export the training data to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "349687e0-f375-4f87-97a8-20ed44711e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the name and location of the output file\n",
    "if subsample:\n",
    "    output_file = os.path.join(\n",
    "        output_folder, 'single_crops_merged_training_features_2022_subsample.csv')\n",
    "else:\n",
    "    output_file = os.path.join(\n",
    "        output_folder, 'single_crops_merged_training_features_2022_all.csv')\n",
    "\n",
    "# convert to a dataframe and save as a csv file\n",
    "model_input_df = pd.DataFrame(model_input, columns=column_names)\n",
    "model_input_df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
