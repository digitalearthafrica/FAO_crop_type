{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e4ea29-d3df-4e0a-a56d-9b8d11fdb5b8",
   "metadata": {},
   "source": [
    "# Create a regional or country-wide crop type map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73bf38-f266-493c-8167-c8fd2542f218",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Once we are satisfied with the map results for the test areas, we can proceed to apply the model over a larger region or an entire country. To limit the memory use, we will break a large area into tiles, make predictions over tiles and merge the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5d332-e41a-45e8-a3b7-54da2112e1bc",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook can be used to generate a crop type map over a region defined in a shapefile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97d7e9-2767-4d58-afd9-02b1c1b9a204",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768ca38-3928-48fd-8be2-625d45dff12e",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdac518-de9e-4e91-8375-ab4972b4e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.3-CAPI-1.16.1). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import datacube\n",
    "import xarray as xr\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils.cog import write_cog\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "from deafrica_tools.plotting import rgb, display_map\n",
    "from deafrica_tools.classification import predict_xr\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "    \n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import geomedian_with_mads, xr_geomedian\n",
    "\n",
    "from feature_collection import feature_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68117e-55a1-4e78-9dd3-8b95f2f7f381",
   "metadata": {},
   "source": [
    "## Create Dask cluster for running predictions\n",
    "\n",
    "We use dask to parallel and speed up the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cf6218-893f-454e-89d0-a3e4e70b3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38755</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/whusggliuqx@gmail.com/proxy/8787/status' target='_blank'>/user/whusggliuqx@gmail.com/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>28.14 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:38755' processes=1 threads=4, memory=28.14 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncpus = round(get_cpu_quota())\n",
    "print(\"ncpus = \" + str(ncpus))\n",
    "\n",
    "# client = create_local_dask_cluster(return_client=True, n_workers=1, threads_per_worker=ncpus)\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679589c0-2b8f-45fb-93e8-07c319a2d251",
   "metadata": {},
   "source": [
    "## Load model and class labels\n",
    "\n",
    "We use the model trained and saved in the [fit classifier notebook](3_Fit_classifier.ipynb). it is important that the list of features used match the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91cdfd9f-c15e-4482-84f5-8ecb3843d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basepath = \"Results/Model/\"\n",
    "\n",
    "# Choose model and load\n",
    "model_path = os.path.join(model_basepath, \"rf_removecorrfeaturesgt0p9_simplified_cv.joblib\")\n",
    "model = load(model_path).set_params(n_jobs=1)\n",
    "\n",
    "\n",
    "# Get label dictionary\n",
    "labels_path = os.path.join(model_basepath, \"class_labels.json\")\n",
    "with open(labels_path, \"r\") as json_file:\n",
    "    labels_dict = json.load(json_file)\n",
    "\n",
    "# Get model features\n",
    "feautres_path = os.path.join(model_basepath, \"rf_removecorrfeaturesgt0p9_simplified_features.json\")\n",
    "with open(feautres_path, \"r\") as json_file:\n",
    "    features_dict = json.load(json_file)\n",
    "    \n",
    "features = features_dict[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac8516a-400c-4930-8726-12d50082e925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c0efc-374c-476e-90a9-339a6ec4441d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load area of interest (AOI)\n",
    "\n",
    "Shapefiles for Mozambique tiles are provided in the `Data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4ec4d8-81bf-4186-8568-590b5d50de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_crs=\"EPSG:6933\"\n",
    "\n",
    "# Choose the test area shapefile\n",
    "AOIs_file = \"Data/Mozambique_tiles_smaller.shp\"\n",
    "AOIs_gdf = gpd.read_file(AOIs_file).to_crs(output_crs)\n",
    "\n",
    "# Set results path\n",
    "output_folder = \"Results/Map\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087450e-ad5c-4017-81f6-f7e8985ddc51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the query for running the predictions\n",
    "\n",
    "We use the query saved from the training data collection notebook to ensure data from the same periods are retrieved. However, only selected features will be used. \n",
    "\n",
    "> We add `dask_chunks` to the query parameter so the data will be lazy-loaded and only the features used by the model will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4829adb4-d90a-4dbe-9119-0251cd79ece9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annual_geomedian_times': {'annual_2021': '2021-01-01'},\n",
       " 'semiannual_geomedian_times': {'semiannual_2021_07': '2021-07-01',\n",
       "  'semiannual_2022_01': '2022-01-01'},\n",
       " 'time_ranges': {'Q4_2021': slice('2021-10-01', '2021-12-31', None),\n",
       "  'Q1_2022': slice('2022-01-01', '2022-03-31', None),\n",
       "  'Q2_2022': slice('2022-04-01', '2022-06-30', None),\n",
       "  'Q3_2022': slice('2022-07-01', '2022-09-30', None)},\n",
       " 'resolution': (-10, 10),\n",
       " 'output_crs': 'EPSG:6933',\n",
       " 'dask_chunks': {'x': 2000, 'y': 2000}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the query used for fitting\n",
    "query_file = os.path.join(model_basepath, \"query.pickle\")\n",
    "\n",
    "with open(query_file, \"rb\") as f:\n",
    "    query = pickle.load(f)\n",
    "    \n",
    "# Specify any specific additions to the data query -- e.g. dask_chunks for enabling parallel computation\n",
    "if ncpus<=4:\n",
    "    dask_chunks = {\"x\": 1500, \"y\": 1500}\n",
    "else:\n",
    "    dask_chunks = {\"x\": 4000, \"y\": 4000}\n",
    "query.update({\"dask_chunks\": dask_chunks})\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9d60e-2c27-44f6-a5c6-b1903b8b5be8",
   "metadata": {},
   "source": [
    "## Apply classification model to the AOI\n",
    "\n",
    "The model will be applied over each tile, producing a prediction map and a probabilities map. The maps are saved as Cloud-Optimized Geotiffs (COGs).\n",
    "\n",
    "> Tiles are processed in sequence. For each tile, the processing needs to fit into the compute resources available in the sandbox. Make the tile size smaller if you run out of memory. For production of a map over a large region or country, consider applying for [a large sandbox (with more CPUs and momery)](\n",
    "https://helpdesk.digitalearthafrica.org/portal/en/community/topic/call-for-application-for-access-to-large-sandboxes-15-processing-cores-and-120-gb-of-memory)\n",
    "\n",
    "If output files for a tile already exist, processing for the tile can be skipped. This is useful if the process fails partway through, or if you are logged out of the sandbox before all tiles are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ce62ea-db2d-4175-a9a8-287a1d9fa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_exisiting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e1527-c548-44ef-991e-b8963a659933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Polygon 0\n",
      "    Loading feature data\n",
      "predicting...\n",
      "   probabilities...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dc = datacube.Datacube(app=\"crop_type_ml\")\n",
    "\n",
    "# for index, aoi in AOIs_gdf.iterrows():\n",
    "for index in range(0,len(AOIs_gdf)):\n",
    "    aoi=AOIs_gdf.iloc[index]\n",
    "    print(f\"Processing Polygon {index}\")\n",
    "    \n",
    "    # Check if polygon has already been processed. If so, skip\n",
    "    output_filename = os.path.join(output_folder, f\"Test_area_{index}_croptype_prediction.tif\")\n",
    "    probabilities_filename = os.path.join(output_folder, f\"Test_area_{index}_croptype_probabilities.tif\")\n",
    "    if skip_exisiting and os.path.exists(output_filename) and os.path.exists(probabilities_filename):\n",
    "         print(\"Completed; Skipping\")\n",
    "         continue\n",
    "\n",
    "    # set up query based on aoi polygon\n",
    "    geom = geometry.Geometry(geom=aoi.geometry, crs=AOIs_gdf.crs)\n",
    "    query.update({\"geopolygon\": geom})\n",
    "\n",
    "    # Load the feature data\n",
    "    print(\"    Loading feature data\")\n",
    "    data = feature_layers(query)\n",
    "#     data = feature_layers(query).persist()\n",
    "    \n",
    "    \n",
    "    # Only keep features that are used by the model\n",
    "    data = data[features]\n",
    "\n",
    "#     data=data.fillna(0)\n",
    "#     #predict using the imported model\n",
    "#     predicted = predict_xr(model,\n",
    "#                            data.unify_chunks(),\n",
    "#                            proba=True,\n",
    "#                            persist=True,\n",
    "#                            clean=True,\n",
    "#                            return_input=False\n",
    "#                           ).astype(np.uint8).persist()\n",
    "    predicted = predict_xr(model,\n",
    "                           data,\n",
    "                           proba=True,\n",
    "                           persist=False,\n",
    "                           clean=True,\n",
    "                           return_input=False\n",
    "                          ).compute().astype(np.uint8)\n",
    "    # Load masks and clip\n",
    "#     crop_mask_query = dict((k,query[k]) for k in ('resolution','output_crs','dask_chunks','geopolygon'))\n",
    "    crop_mask_query = dict((k,query[k]) for k in ('resolution','output_crs','geopolygon'))\n",
    "    crop_mask_query.update({\"time\": \"2019\"})\n",
    "\n",
    "    # Load the crop mask\n",
    "    print(\"    Loading crop_mask\")\n",
    "    crop_mask = dc.load(product=\"crop_mask\", **crop_mask_query)\n",
    "    \n",
    "    # Create a mask for the aoi\n",
    "    print(\"    Getting AOI mask\")\n",
    "    aoi_mask = xr_rasterize(\n",
    "        gdf=gpd.GeoDataFrame({\"Polygon\": [index], \"geometry\": [aoi.geometry]}, crs=AOIs_gdf.crs),\n",
    "        da=predicted,\n",
    "        transform=predicted.geobox.transform,\n",
    "        crs=output_crs,\n",
    "    )\n",
    "\n",
    "    # set the no data value\n",
    "    NODATA = 255\n",
    "\n",
    "    # Mask the predictions to\n",
    "    print(\"    Preparing predictions\")\n",
    "    predicted_masked = (\n",
    "#         predicted.Predictions.where((crop_mask.filtered == 1) & (aoi_mask==1), NODATA)\n",
    "        predicted.Predictions.where((crop_mask.mask == 1) & (aoi_mask==1), NODATA) # revised: using unfiltered crop mask instead\n",
    "    )\n",
    "    \n",
    "    predicted_masked.attrs[\"nodata\"] = NODATA\n",
    "    \n",
    "    # Write to cog\n",
    "    print(f\"    Writing predictions to {output_filename}\")\n",
    "    write_cog(\n",
    "        predicted_masked,\n",
    "        fname=output_filename,\n",
    "        overwrite=True,\n",
    "        nodata=255,\n",
    "    )\n",
    "    \n",
    "    del predicted_masked\n",
    "    \n",
    "    probability_masked = (\n",
    "#         predicted.Probabilities.where((crop_mask.filtered == 1) & (aoi_mask==1), NODATA)\n",
    "        predicted.Probabilities.where((crop_mask.mask == 1) & (aoi_mask==1), NODATA) # revised: using unfiltered crop mask instead\n",
    "    )\n",
    "    \n",
    "    probability_masked.attrs[\"nodata\"] = NODATA\n",
    "    \n",
    "    print(f\"    Writing probabilities to {probabilities_filename}\")\n",
    "    write_cog(\n",
    "        probability_masked,\n",
    "        fname=probabilities_filename,\n",
    "        overwrite=True,\n",
    "        nodata=255,\n",
    "    )\n",
    "    \n",
    "    del probability_masked\n",
    "    \n",
    "    del crop_mask\n",
    "    del aoi_mask\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad2c2d",
   "metadata": {},
   "source": [
    "## Mosaic maps\n",
    "\n",
    "The tiled maps merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4afe1-1901-4023-a41a-1230d6e84711",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gdal_merge.py -o results/Test_areas_mosaic_croptype_merged_prediction.tif -co COMPRESS=Deflate -ot Byte results/Test_area_*_croptype_prediction.tif -init 255 -a_nodata 255\n",
    "! gdal_merge.py -o results/Test_areas_mosaic_croptype_merged_probabilities.tif -co COMPRESS=Deflate -ot Byte results/Test_area_*_croptype_probabilities.tif -init 255 -a_nodata 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e840ff7-58fc-4474-9161-d32c2d6a9c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
