{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdac518-de9e-4e91-8375-ab4972b4e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.3-CAPI-1.16.1). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import datacube\n",
    "import xarray as xr\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils.cog import write_cog\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "from deafrica_tools.plotting import rgb, display_map\n",
    "from deafrica_tools.classification import predict_xr\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "    \n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import geomedian_with_mads, xr_geomedian\n",
    "\n",
    "from feature_collection import feature_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68117e-55a1-4e78-9dd3-8b95f2f7f381",
   "metadata": {},
   "source": [
    "## Create Dask cluster for running predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cf6218-893f-454e-89d0-a3e4e70b3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 62\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38831</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/whusggliuqx@gmail.com/proxy/8787/status' target='_blank'>/user/whusggliuqx@gmail.com/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>62</li>\n",
       "  <li><b>Memory: </b>512.40 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:38831' processes=1 threads=62, memory=512.40 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncpus = round(get_cpu_quota())\n",
    "print(\"ncpus = \" + str(ncpus))\n",
    "\n",
    "# client = create_local_dask_cluster(return_client=True, n_workers=1, threads_per_worker=ncpus)\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679589c0-2b8f-45fb-93e8-07c319a2d251",
   "metadata": {},
   "source": [
    "## Read in training data feaure names and class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f8e4e2-b8fd-4637-9c89-f60bcee97a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name = \"exp_multipixel_allfeatures_removecorrfeaturesgt0p9_RandomForest\"\n",
    "experiment_name=\"rf_removecorrfeaturesgt0p9_simplified_cv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cdfd9f-c15e-4482-84f5-8ecb3843d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label dictionary\n",
    "labels_path = \"results/class_labels.json\"\n",
    "with open(labels_path, \"r\") as json_file:\n",
    "    labels_dict = json.load(json_file)\n",
    "\n",
    "# Get model features\n",
    "feautres_path = f\"results/{experiment_name}_features.json\"\n",
    "with open(feautres_path, \"r\") as json_file:\n",
    "    features_dict = json.load(json_file)\n",
    "    \n",
    "features = features_dict[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac8516a-400c-4930-8726-12d50082e925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c0efc-374c-476e-90a9-339a6ec4441d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load trained ML model and shapefile for prediction\n",
    "\n",
    "To manage memory, we provide a shapefile that splits the area of interest into tiles, which are then looped over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4ec4d8-81bf-4186-8568-590b5d50de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model and load\n",
    "model_path = f\"results/{experiment_name}.joblib\"\n",
    "model = load(model_path).set_params(n_jobs=1)\n",
    "\n",
    "output_crs=\"EPSG:32736\"\n",
    "# Choose file containing test areas and load\n",
    "# AOIs_file = \"data/Random_Squares_10km_within_AOI_Crop_subset.shp\"\n",
    "# AOIs_file = \"data/AOI_Crop.shp\"\n",
    "# AOIs_file = \"data/Crop_type_test_areas.shp\"\n",
    "AOIs_file = \"data/Crop_type_test_areas_3_larger.shp\"\n",
    "AOIs_gdf = gpd.read_file(AOIs_file).to_crs(output_crs)\n",
    "\n",
    "# Set results path\n",
    "results_path = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a94503-f818-46ba-b4fe-5757f5ec47b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((520103.252 7850182.937, 520126.356 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((548162.280 7222518.913, 548232.887 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((1119729.592 8517666.380, 1120258.862...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           geometry\n",
       "0  None  POLYGON ((520103.252 7850182.937, 520126.356 7...\n",
       "1  None  POLYGON ((548162.280 7222518.913, 548232.887 7...\n",
       "2  None  POLYGON ((1119729.592 8517666.380, 1120258.862..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AOIs_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087450e-ad5c-4017-81f6-f7e8985ddc51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the query for running the predictions\n",
    "This uses the existing query from the training data collection notebook, and adds `dask_chunks` as an additional parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4829adb4-d90a-4dbe-9119-0251cd79ece9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annual_geomedian_times': {'annual_2021': '2021-01-01'},\n",
       " 'semiannual_geomedian_times': {'semiannual_2021_07': '2021-07-01',\n",
       "  'semiannual_2022_01': '2022-01-01'},\n",
       " 'time_ranges': {'Q4_2021': slice('2021-10-01', '2021-12-31', None),\n",
       "  'Q1_2022': slice('2022-01-01', '2022-03-31', None),\n",
       "  'Q2_2022': slice('2022-04-01', '2022-06-30', None),\n",
       "  'Q3_2022': slice('2022-07-01', '2022-09-30', None)},\n",
       " 'resolution': (-10, 10),\n",
       " 'output_crs': 'EPSG:32736',\n",
       " 'dask_chunks': {'x': 4000, 'y': 4000}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the query used for fitting\n",
    "query_file = \"results/query.pickle\"\n",
    "\n",
    "with open(query_file, \"rb\") as f:\n",
    "    query = pickle.load(f)\n",
    "    \n",
    "# Specify any specific additions to the data query -- e.g. dask_chunks for enabling parallel computation\n",
    "dask_chunks = {\"x\": 4000, \"y\": 4000}\n",
    "query.update({\"dask_chunks\": dask_chunks})\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9d60e-2c27-44f6-a5c6-b1903b8b5be8",
   "metadata": {},
   "source": [
    "## Run model AOIs\n",
    "\n",
    "The model will be run for each area of the shapefile, producing a prediction file and a probabilities file. These will be saved to the results folder.\n",
    "\n",
    "If an area has already been processed, it will be skipped, and prediction will resume for any incomplete tiles. This is useful if the process fails partway through, or if you are logged out of the sandbox before completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17e1527-c548-44ef-991e-b8963a659933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Polygon 0\n",
      "    Loading feature data\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading crop_mask\n",
      "    Getting AOI mask\n",
      "    Preparing predictions\n",
      "    Writing predictions to results//Test_area_0_croptype_prediction.tif\n",
      "    Writing probabilities to results//Test_area_0_croptype_probabilities.tif\n",
      "Processing Polygon 1\n",
      "    Loading feature data\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading crop_mask\n",
      "    Getting AOI mask\n",
      "    Preparing predictions\n",
      "    Writing predictions to results//Test_area_1_croptype_prediction.tif\n",
      "    Writing probabilities to results//Test_area_1_croptype_probabilities.tif\n",
      "Processing Polygon 2\n",
      "    Loading feature data\n",
      "predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading crop_mask\n",
      "    Getting AOI mask\n",
      "    Preparing predictions\n",
      "    Writing predictions to results//Test_area_2_croptype_prediction.tif\n",
      "    Writing probabilities to results//Test_area_2_croptype_probabilities.tif\n",
      "CPU times: user 20min 18s, sys: 1min 3s, total: 21min 22s\n",
      "Wall time: 4h 11min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dc = datacube.Datacube(app=\"crop_type_ml\")\n",
    "\n",
    "# for index, aoi in AOIs_gdf.iterrows():\n",
    "for index in range(0,len(AOIs_gdf)):\n",
    "    aoi=AOIs_gdf.iloc[index]\n",
    "    print(f\"Processing Polygon {index}\")\n",
    "    \n",
    "    # Check if polygon has already been processed. If so, skip\n",
    "    output_filename = f\"{results_path}/Test_area_{index}_croptype_prediction.tif\"\n",
    "#     if os.path.exists(output_filename):\n",
    "#         print(\"Completed; Skipping\")\n",
    "#         continue\n",
    "\n",
    "    # set up query based on aoi polygon\n",
    "    geom = geometry.Geometry(geom=aoi.geometry, crs=AOIs_gdf.crs)\n",
    "    query.update({\"geopolygon\": geom})\n",
    "\n",
    "    # Load the feature data\n",
    "    print(\"    Loading feature data\")\n",
    "    data = feature_layers(query)\n",
    "#     data = feature_layers(query).persist()\n",
    "    \n",
    "    \n",
    "    # Only keep features that are used by the model\n",
    "    data = data[features]\n",
    "\n",
    "#     data=data.fillna(0)\n",
    "#     #predict using the imported model\n",
    "#     predicted = predict_xr(model,\n",
    "#                            data.unify_chunks(),\n",
    "#                            proba=True,\n",
    "#                            persist=True,\n",
    "#                            clean=True,\n",
    "#                            return_input=False\n",
    "#                           ).astype(np.uint8).persist()\n",
    "    predicted = predict_xr(model,\n",
    "                           data,\n",
    "                           proba=True,\n",
    "                           persist=False,\n",
    "                           clean=True,\n",
    "                           return_input=False\n",
    "                          ).compute().astype(np.uint8)\n",
    "    # Load masks and clip\n",
    "#     crop_mask_query = dict((k,query[k]) for k in ('resolution','output_crs','dask_chunks','geopolygon'))\n",
    "    crop_mask_query = dict((k,query[k]) for k in ('resolution','output_crs','geopolygon'))\n",
    "    crop_mask_query.update({\"time\": \"2019\"})\n",
    "\n",
    "    # Load the crop mask\n",
    "    print(\"    Loading crop_mask\")\n",
    "    crop_mask = dc.load(product=\"crop_mask\", **crop_mask_query)\n",
    "    \n",
    "    # Create a mask for the aoi\n",
    "    print(\"    Getting AOI mask\")\n",
    "    aoi_mask = xr_rasterize(\n",
    "        gdf=gpd.GeoDataFrame({\"Polygon\": [index], \"geometry\": [aoi.geometry]}, crs=AOIs_gdf.crs),\n",
    "        da=predicted,\n",
    "        transform=predicted.geobox.transform,\n",
    "        crs=output_crs,\n",
    "    )\n",
    "\n",
    "    # set the no data value\n",
    "    NODATA = 255\n",
    "\n",
    "    # Mask the predictions to\n",
    "    print(\"    Preparing predictions\")\n",
    "    predicted_masked = (\n",
    "#         predicted.Predictions.where((crop_mask.filtered == 1) & (aoi_mask==1), NODATA)\n",
    "        predicted.Predictions.where((crop_mask.mask == 1) & (aoi_mask==1), NODATA) # revised: using unfiltered crop mask instead\n",
    "    )\n",
    "    \n",
    "    predicted_masked.attrs[\"nodata\"] = NODATA\n",
    "    \n",
    "    # Write to cog\n",
    "    prediction_file = f\"{results_path}/Test_area_{index}_croptype_prediction.tif\"\n",
    "    print(f\"    Writing predictions to {prediction_file}\")\n",
    "    write_cog(\n",
    "        predicted_masked,\n",
    "        fname=prediction_file,\n",
    "        overwrite=True,\n",
    "        nodata=255,\n",
    "    )\n",
    "    \n",
    "    del predicted_masked\n",
    "    \n",
    "    probability_masked = (\n",
    "#         predicted.Probabilities.where((crop_mask.filtered == 1) & (aoi_mask==1), NODATA)\n",
    "        predicted.Probabilities.where((crop_mask.mask == 1) & (aoi_mask==1), NODATA) # revised: using unfiltered crop mask instead\n",
    "    )\n",
    "    \n",
    "    probability_masked.attrs[\"nodata\"] = NODATA\n",
    "    \n",
    "    probabilities_file = f\"{results_path}/Test_area_{index}_croptype_probabilities.tif\"\n",
    "    print(f\"    Writing probabilities to {probabilities_file}\")\n",
    "    write_cog(\n",
    "        probability_masked,\n",
    "        fname=probabilities_file,\n",
    "        overwrite=True,\n",
    "        nodata=255,\n",
    "    )\n",
    "    \n",
    "    del probability_masked\n",
    "    \n",
    "    del crop_mask\n",
    "    del aoi_mask\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a4afe1-1901-4023-a41a-1230d6e84711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "! gdal_merge.py -o results/Test_areas_mosaic_croptype_merged_prediction.tif -co COMPRESS=Deflate -ot Byte results/Test_area_*_croptype_prediction.tif -init 255 -a_nodata 255\n",
    "! gdal_merge.py -o results/Test_areas_mosaic_croptype_merged_probabilities.tif -co COMPRESS=Deflate -ot Byte results/Test_area_*_croptype_probabilities.tif -init 255 -a_nodata 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e840ff7-58fc-4474-9161-d32c2d6a9c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
