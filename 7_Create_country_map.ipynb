{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e4ea29-d3df-4e0a-a56d-9b8d11fdb5b8",
   "metadata": {},
   "source": [
    "# Create a regional or country-wide crop type map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73bf38-f266-493c-8167-c8fd2542f218",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Once we are satisfied with the map results for the test areas, we can proceed to apply the model over a larger region or an entire country. To limit the memory use, we will break a large area into tiles, make predictions over tiles and merge the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5d332-e41a-45e8-a3b7-54da2112e1bc",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook can be used to generate a crop type map over a region defined in a shapefile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97d7e9-2767-4d58-afd9-02b1c1b9a204",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768ca38-3928-48fd-8be2-625d45dff12e",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efdac518-de9e-4e91-8375-ab4972b4e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "import xarray as xr\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils.cog import write_cog\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "from deafrica_tools.plotting import rgb, display_map\n",
    "from deafrica_tools.classification import predict_xr\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "    \n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import geomedian_with_mads, xr_geomedian\n",
    "\n",
    "from feature_collection import feature_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68117e-55a1-4e78-9dd3-8b95f2f7f381",
   "metadata": {},
   "source": [
    "## Create Dask cluster for running predictions\n",
    "\n",
    "We use dask to parallel and speed up the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cf6218-893f-454e-89d0-a3e4e70b3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45141 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:34297</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/fang.yuan@digitalearthafrica.org/proxy/45141/status' target='_blank'>/user/fang.yuan@digitalearthafrica.org/proxy/45141/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>28.14 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:34297' processes=1 threads=4, memory=28.14 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncpus = round(get_cpu_quota())\n",
    "print(\"ncpus = \" + str(ncpus))\n",
    "\n",
    "# client = create_local_dask_cluster(return_client=True, n_workers=1, threads_per_worker=ncpus)\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679589c0-2b8f-45fb-93e8-07c319a2d251",
   "metadata": {},
   "source": [
    "## Load model and class labels\n",
    "\n",
    "We use the model trained and saved in the [fit classifier notebook](3_Fit_classifier.ipynb). it is important that the list of features used match the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91cdfd9f-c15e-4482-84f5-8ecb3843d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model and load\n",
    "model_path = f\"Results/rf_removecorrfeaturesgt0p9_simplified_cv.joblib\"\n",
    "model = load(model_path).set_params(n_jobs=1)\n",
    "\n",
    "\n",
    "# Get label dictionary\n",
    "labels_path = \"Results/class_labels.json\"\n",
    "with open(labels_path, \"r\") as json_file:\n",
    "    labels_dict = json.load(json_file)\n",
    "\n",
    "# Get model features\n",
    "feautres_path = f\"Results/rf_removecorrfeaturesgt0p9_simplified_features.json\"\n",
    "with open(feautres_path, \"r\") as json_file:\n",
    "    features_dict = json.load(json_file)\n",
    "    \n",
    "features = features_dict[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac8516a-400c-4930-8726-12d50082e925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c0efc-374c-476e-90a9-339a6ec4441d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4ec4d8-81bf-4186-8568-590b5d50de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_crs=\"EPSG:6933\"\n",
    "# Choose file containing test areas and load\n",
    "# AOIs_file = \"data/Random_Squares_10km_within_AOI_Crop_subset.shp\"\n",
    "# AOIs_file = \"data/AOI_Crop.shp\"\n",
    "# AOIs_file = \"data/Crop_type_test_areas.shp\"\n",
    "AOIs_file = \"Data/Crop_type_test_areas_3_larger.shp\"\n",
    "AOIs_gdf = gpd.read_file(AOIs_file).to_crs(output_crs)\n",
    "\n",
    "# Set results path\n",
    "results_path = \"Results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087450e-ad5c-4017-81f6-f7e8985ddc51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the query for running the predictions\n",
    "\n",
    "We use the query saved from the training data collection notebook to ensure data from the same periods are retrieved. However, only selected features will be used. \n",
    "\n",
    "> We add `dask_chunks` to the query parameter so the data will be lazy-loaded and only the features used by the model will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4829adb4-d90a-4dbe-9119-0251cd79ece9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annual_geomedian_times': {'annual_2021': '2021-01-01'},\n",
       " 'semiannual_geomedian_times': {'semiannual_2021_07': '2021-07-01',\n",
       "  'semiannual_2022_01': '2022-01-01'},\n",
       " 'time_ranges': {'Q4_2021': slice('2021-10-01', '2021-12-31', None),\n",
       "  'Q1_2022': slice('2022-01-01', '2022-03-31', None),\n",
       "  'Q2_2022': slice('2022-04-01', '2022-06-30', None),\n",
       "  'Q3_2022': slice('2022-07-01', '2022-09-30', None)},\n",
       " 'resolution': (-10, 10),\n",
       " 'output_crs': 'EPSG:6933',\n",
       " 'dask_chunks': {'x': 4000, 'y': 4000}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the query used for fitting\n",
    "query_file = \"Results/query.pickle\"\n",
    "\n",
    "with open(query_file, \"rb\") as f:\n",
    "    query = pickle.load(f)\n",
    "    \n",
    "# Specify any specific additions to the data query -- e.g. dask_chunks for enabling parallel computation\n",
    "dask_chunks = {\"x\": 4000, \"y\": 4000}\n",
    "query.update({\"dask_chunks\": dask_chunks})\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9d60e-2c27-44f6-a5c6-b1903b8b5be8",
   "metadata": {},
   "source": [
    "## Apply classification model to the AOI\n",
    "\n",
    "The model will be applied over each tile, producing a prediction map and a probabilities map. The maps are saved as Cloud-Optimized Geotiffs (COGs).\n",
    "\n",
    "If output files for a tile already exist, processing for the tile will be skipped. This is useful if the process fails partway through, or if you are logged out of the sandbox before all tiles are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e1527-c548-44ef-991e-b8963a659933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dc = datacube.Datacube(app=\"crop_type_ml\")\n",
    "\n",
    "# for index, aoi in AOIs_gdf.iterrows():\n",
    "for index in range(0,len(AOIs_gdf)):\n",
    "    aoi=AOIs_gdf.iloc[index]\n",
    "    print(f\"Processing Polygon {index}\")\n",
    "    \n",
    "    # Check if polygon has already been processed. If so, skip\n",
    "    output_filename = f\"{results_path}/Test_area_{index}_croptype_prediction.tif\"\n",
    "#     if os.path.exists(output_filename):\n",
    "#         print(\"Completed; Skipping\")\n",
    "#         continue\n",
    "\n",
    "    # set up query based on aoi polygon\n",
    "    geom = geometry.Geometry(geom=aoi.geometry, crs=AOIs_gdf.crs)\n",
    "    query.update({\"geopolygon\": geom})\n",
    "\n",
    "    # Load the feature data\n",
    "    print(\"    Loading feature data\")\n",
    "    data = feature_layers(query)\n",
    "#     data = feature_layers(query).persist()\n",
    "    \n",
    "    \n",
    "    # Only keep features that are used by the model\n",
    "    data = data[features]\n",
    "\n",
    "#     data=data.fillna(0)\n",
    "#     #predict using the imported model\n",
    "#     predicted = predict_xr(model,\n",
    "#                            data.unify_chunks(),\n",
    "#                            proba=True,\n",
    "#                            persist=True,\n",
    "#                            clean=True,\n",
    "#                            return_input=False\n",
    "#                           ).astype(np.uint8).persist()\n",
    "    predicted = predict_xr(model,\n",
    "                           data,\n",
    "                           proba=True,\n",
    "                           persist=False,\n",
    "                           clean=True,\n",
    "                           return_input=False\n",
    "                          ).compute().astype(np.uint8)\n",
    "    # Load masks and clip\n",
    "#     crop_mask_query = dict((k,query[k]) for k in ('resolution','output_crs','dask_chunks','geopolygon'))\n",
    "    crop_mask_query = dict((k,query[k]) for k in ('resolution','output_crs','geopolygon'))\n",
    "    crop_mask_query.update({\"time\": \"2019\"})\n",
    "\n",
    "    # Load the crop mask\n",
    "    print(\"    Loading crop_mask\")\n",
    "    crop_mask = dc.load(product=\"crop_mask\", **crop_mask_query)\n",
    "    \n",
    "    # Create a mask for the aoi\n",
    "    print(\"    Getting AOI mask\")\n",
    "    aoi_mask = xr_rasterize(\n",
    "        gdf=gpd.GeoDataFrame({\"Polygon\": [index], \"geometry\": [aoi.geometry]}, crs=AOIs_gdf.crs),\n",
    "        da=predicted,\n",
    "        transform=predicted.geobox.transform,\n",
    "        crs=output_crs,\n",
    "    )\n",
    "\n",
    "    # set the no data value\n",
    "    NODATA = 255\n",
    "\n",
    "    # Mask the predictions to\n",
    "    print(\"    Preparing predictions\")\n",
    "    predicted_masked = (\n",
    "#         predicted.Predictions.where((crop_mask.filtered == 1) & (aoi_mask==1), NODATA)\n",
    "        predicted.Predictions.where((crop_mask.mask == 1) & (aoi_mask==1), NODATA) # revised: using unfiltered crop mask instead\n",
    "    )\n",
    "    \n",
    "    predicted_masked.attrs[\"nodata\"] = NODATA\n",
    "    \n",
    "    # Write to cog\n",
    "    prediction_file = f\"{results_path}/Test_area_{index}_croptype_prediction.tif\"\n",
    "    print(f\"    Writing predictions to {prediction_file}\")\n",
    "    write_cog(\n",
    "        predicted_masked,\n",
    "        fname=prediction_file,\n",
    "        overwrite=True,\n",
    "        nodata=255,\n",
    "    )\n",
    "    \n",
    "    del predicted_masked\n",
    "    \n",
    "    probability_masked = (\n",
    "#         predicted.Probabilities.where((crop_mask.filtered == 1) & (aoi_mask==1), NODATA)\n",
    "        predicted.Probabilities.where((crop_mask.mask == 1) & (aoi_mask==1), NODATA) # revised: using unfiltered crop mask instead\n",
    "    )\n",
    "    \n",
    "    probability_masked.attrs[\"nodata\"] = NODATA\n",
    "    \n",
    "    probabilities_file = f\"{results_path}/Test_area_{index}_croptype_probabilities.tif\"\n",
    "    print(f\"    Writing probabilities to {probabilities_file}\")\n",
    "    write_cog(\n",
    "        probability_masked,\n",
    "        fname=probabilities_file,\n",
    "        overwrite=True,\n",
    "        nodata=255,\n",
    "    )\n",
    "    \n",
    "    del probability_masked\n",
    "    \n",
    "    del crop_mask\n",
    "    del aoi_mask\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad2c2d",
   "metadata": {},
   "source": [
    "## Mosaic maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a4afe1-1901-4023-a41a-1230d6e84711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "! gdal_merge.py -o results/Test_areas_mosaic_croptype_merged_prediction.tif -co COMPRESS=Deflate -ot Byte results/Test_area_*_croptype_prediction.tif -init 255 -a_nodata 255\n",
    "! gdal_merge.py -o results/Test_areas_mosaic_croptype_merged_probabilities.tif -co COMPRESS=Deflate -ot Byte results/Test_area_*_croptype_probabilities.tif -init 255 -a_nodata 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e840ff7-58fc-4474-9161-d32c2d6a9c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
